{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10618_HW4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arndmghsh/TopicModelling-GaussianLDA-GibbsSampler/blob/master/glda_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFzjsyoBV-Jc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy.stats import dirichlet, multivariate_normal, multinomial\n",
        "from utils import *\n",
        "import heapq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_6LLFzzuDAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GaussianLDA(object):\n",
        "    \"\"\"Object that encapsulates Gaussian LDA algorithm\"\"\"\n",
        "    def __init__(self, n_topics):\n",
        "        #Set up variables used throughout process.\n",
        "        self.n_topics = n_topics\n",
        "        self.embeddings, self.word2index = load_embeddings()\n",
        "        self.index2word = {self.word2index[w]:w for w in self.word2index}\n",
        "        self.corpus = load_corpus()\n",
        "        print(self.corpus.shape)\n",
        "\n",
        "        self.D = self.corpus.shape[0]\n",
        "        self.V = self.corpus.shape[1]\n",
        "        self.M = 50\n",
        "        #Randomly assign words to topics\n",
        "        self.z_di = {}\n",
        "        #Count document / topic coocurrences \n",
        "        self.Nk = np.zeros(n_topics)\n",
        "        self.vk_bar = np.zeros((n_topics, self.M))\n",
        "        self.n_kd = np.zeros((n_topics, self.D))\n",
        "        for d in range(self.D):\n",
        "            for i in range(self.V):\n",
        "                self.z_di['d'+str(d)+'i'+str(i)] = []\n",
        "                for w in range(int(self.corpus[d][i])):\n",
        "                    k = np.random.randint(n_topics)\n",
        "                    self.z_di['d'+str(d)+'i'+str(i)].append(k)\n",
        "                    self.Nk[k] += 1\n",
        "                    self.vk_bar[k] += self.embeddings[i]\n",
        "                    self.n_kd[k][d] += 1\n",
        "\n",
        "        #Set up prior parameters\n",
        "        self.mu = np.zeros(self.M)\n",
        "        # self.mu = np.sum(self.embeddings, axis=0)/self.V\n",
        "        \n",
        "        self.kappa = 0.01\n",
        "        self.Sigma = np.identity(self.M)\n",
        "        self.nu = self.M\n",
        "        # for each topic\n",
        "        self.mu_k = np.zeros((n_topics, self.M))\n",
        "        self.nu_k = np.zeros(n_topics)\n",
        "        self.kappa_k = np.zeros(n_topics)\n",
        "        \n",
        "        #Set up topic parameters      \n",
        "        self.alpha = 10*np.ones(n_topics)\n",
        "        # Update topic parameters\n",
        "        for k in range(n_topics):\n",
        "            self.update_topic_parameters(k)\n",
        "\n",
        "    def log_likelihood(self):\n",
        "        #Return the log-likelihood of the joint assignment of topics and words under G-LDA\n",
        "        logll = 0\n",
        "        for d in range(self.D):\n",
        "            # calculate p_theta_d for each document only once\n",
        "            theta_d = self.n_kd[:,d]/(np.sum(self.n_kd[:,d]))\n",
        "            log_p_theta_d = dirichlet.logpdf(theta_d, self.alpha)\n",
        "            logll += log_p_theta_d\n",
        "            for i in range(self.V):\n",
        "                for w in range(int(self.corpus[d][i])):\n",
        "                    # Topic assigned\n",
        "                    k = self.z_di['d'+str(d)+'i'+str(i)][w]\n",
        "                    # # calculate p_theta_d\n",
        "                    # theta_d = self.n_kd[:,d]/(np.sum(self.n_kd[:,d]))\n",
        "                    # log_p_theta_d = dirichlet.logpdf(theta_d, self.alpha)\n",
        "\n",
        "                    # # calculate p_mu_k\n",
        "                    # log_p_mu_k = multivariate_normal.logpdf(self.mu_k[k], self.mu, \\\n",
        "                    #                 (1/self.kappa)*np.identity(self.M))\n",
        "                    \n",
        "                    # calculate p_zk\n",
        "                    log_p_zk = np.log(theta_d[k])\n",
        "                    # calculate p_v_di\n",
        "                    log_p_v_di = multivariate_normal.logpdf(self.embeddings[i], self.mu_k[k], \\\n",
        "                                                                        np.identity(self.M))\n",
        "                    # logll += log_p_theta_d + log_p_mu_k + log_p_zk + log_p_v_di\n",
        "                    logll +=  log_p_zk + log_p_v_di\n",
        "        \n",
        "        # calculate p_mu_k for each topic only once\n",
        "        for k in range(self.n_topics):\n",
        "            log_p_mu_k = multivariate_normal.logpdf(self.mu_k[k], self.mu, \\\n",
        "                                    (1/self.kappa)*np.identity(self.M))\n",
        "            logll += log_p_mu_k\n",
        "        \n",
        "        return logll\n",
        "\n",
        "    def update_topic_parameters(self, k):\n",
        "        #Update topic parameters for topic k\n",
        "        self.kappa_k[k] = self.kappa + self.Nk[k]\n",
        "        self.nu_k[k] = self.nu + self.Nk[k]\n",
        "        self.mu_k[k] = (self.kappa*self.mu + self.vk_bar[k])/(self.kappa_k[k])\n",
        "        \n",
        "    def print_top(self, n=10):\n",
        "\n",
        "        log_probs = np.zeros((self.V, self.n_topics))\n",
        "        for i in range(self.V):\n",
        "            for k in range(self.n_topics):\n",
        "                degs_freedom = self.nu_k[k] - self.M + 1\n",
        "                log_probs[i,k] = multivariate_t_distribution(self.embeddings[i], self.mu_k[k,:], degs_freedom, self.M) \n",
        "\n",
        "        log_probs -= np.max(log_probs, axis=1).reshape(-1,1)\n",
        "        log_probs = np.exp(log_probs)\n",
        "        log_probs /= np.sum(log_probs, axis=1).reshape(-1,1)\n",
        "    \n",
        "        top_words_order = np.argsort(-1*log_probs, axis=0)\n",
        "        top_words = []\n",
        "        for k in range(self.n_topics):\n",
        "            top_words_k = [self.index2word[indx] for indx in top_words_order[0:n,k]]\n",
        "            top_words.append(top_words_k)\n",
        "            print(top_words_k)\n",
        "        \n",
        "        return None\n",
        "\n",
        "    def sample(self):\n",
        "        #Do calculation of parameters and sample from posterior\n",
        "        for d in range(self.D):\n",
        "            for i in range(self.V):\n",
        "                for w in range(int(self.corpus[d][i])):\n",
        "                    # current topic\n",
        "                    k = self.z_di['d'+str(d)+'i'+str(i)][w]\n",
        "                    # Adjust statistics\n",
        "                    self.Nk[k]-=1\n",
        "                    self.vk_bar[k] -= self.embeddings[i]\n",
        "                    self.n_kd[k][d] -= 1\n",
        "                    # Update topic parameters\n",
        "                    self.update_topic_parameters(k)\n",
        "                    # Calculate full conditional for each topic\n",
        "                    maxlogp = float('-inf')\n",
        "                    ptilde_k = np.zeros(self.n_topics)\n",
        "                    for k in range(self.n_topics):\n",
        "                        ptilde_k[k] = ((self.n_kd[k][d]+ self.alpha[k])*\n",
        "                                            multivariate_t_distribution(self.embeddings[i,:], \n",
        "                                                self.mu_k[k,:], self.nu_k[k]-self.M+1, self.M))\n",
        "                        maxlogp = max(maxlogp, ptilde_k[k])\n",
        "                    # Normalize to get the Prob dist over topics\n",
        "                    for k in range(self.n_topics):\n",
        "                        ptilde_k[k]-=maxlogp\n",
        "                    # Convert Log to Linear scale\n",
        "                    ptilde_k = np.exp(ptilde_k)\n",
        "                    ptilde_k+= 1e-7\n",
        "                    ptilde_k /= np.sum(ptilde_k)\n",
        "                    # Assign the topic sampled from this multinomial over k\n",
        "                    new_k = np.argmax(multinomial.rvs(1, ptilde_k))\n",
        "                    self.z_di['d'+str(d)+'i'+str(i)][w] = new_k\n",
        "                    # Adjust statistics\n",
        "                    self.Nk[new_k] += 1\n",
        "                    self.vk_bar[new_k] += self.embeddings[i]\n",
        "                    self.n_kd[new_k][d] += 1\n",
        "\n",
        "        return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2pOTvqDuGuK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0c24b5a1-d7ca-4ff8-d073-e96c004cb0db"
      },
      "source": [
        "glda = GaussianLDA(5)\n",
        "n_itr = 50\n",
        "for itr in range(n_itr):\n",
        "    glda.sample()\n",
        "    print(itr)\n",
        "    if itr%10==0:\n",
        "        logll = glda.log_likelihood()\n",
        "        print(itr, ':', logll)\n",
        "logll = glda.log_likelihood()\n",
        "print(itr, ':', logll)\n",
        "\n",
        "glda.print_top()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading embeddings...\n",
            "(3603, 50)\n",
            "Loading corpus...\n",
            "(596, 3603)\n",
            "0\n",
            "0 : -2840452.0224716566\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "10 : -2791240.463436879\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "20 : -2788309.011919851\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "30 : -2787717.6172379516\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "40 : -2787429.79639926\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "49 : -2787282.5795358713\n",
            "['research', 'organization', 'organizations', 'institute', 'education', 'sciences', 'agency', 'council', 'cooperation', 'department']\n",
            "['informatik', 'obo', 'wisc', 'exe', 'acsu', 'centris', 'nntp', 'prb', 'txt', 'cco']\n",
            "['vehicles', 'cars', 'vehicle', 'truck', 'tanks', 'planes', 'engine', 'passenger', 'engines', 'metal']\n",
            "['absolutely', 'honest', 'sense', 'feel', 'understand', 'reasonable', 'things', 'circumstances', 'deserve', 'terribly']\n",
            "['mike', 'scored', 'coach', 'chris', 'steve', 'james', 'brian', 'kevin', 'john', 'joe']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}